{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import POSKeepFilter\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "%matplotlib inline\n",
    "\n",
    "class Word: ##一文の解析\n",
    "    def __init__(self,Text):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        ##text2df\n",
    "        text = Text.replace('\\n','').replace(' ','')\n",
    "        t = Tokenizer()\n",
    "        tokens = t.tokenize(text)\n",
    "        lis = []\n",
    "        for n in tokens :\n",
    "            word = n.surface                             ## 表層形\n",
    "            part_of_speech = n.part_of_speech.split(',') ## 品詞 [品詞,品詞細分類1,品詞細分類2,品詞細分類3]\n",
    "            infl_type = n.infl_type                      ## 活用型\n",
    "            infl_form = n.infl_form                      ## 活用形\n",
    "            base_form = n.base_form                      ## 基本型\n",
    "            reading = n.reading                          ## 読み\n",
    "            phonetic = n.phonetic                        ## 発音\n",
    "            lis.append([word, part_of_speech[0], part_of_speech[1], part_of_speech[2], part_of_speech[3], \n",
    "                        infl_type, infl_form, base_form, reading, phonetic,len(word), len(reading)])\n",
    "        self.df = pd.DataFrame(lis)\n",
    "        self.df.columns = ['word', 'part_of_speech', 'type1', 'type2', 'type3',\n",
    "                      'infl_type', 'infl_form', 'base_form', 'reading', 'phonetic',\n",
    "                      'number_of_word', 'number_of_reading']\n",
    "        \n",
    "    def output_df(self):\n",
    "        return self.df\n",
    "        \n",
    "    def part_s(self):\n",
    "        self._part_s = list(self.df[self.df.part_of_speech == '名詞'].word)\n",
    "        return self._part_s\n",
    "    def part_v(self):\n",
    "        self._part_v = list(self.df[self.df.part_of_speech == '動詞'].word)\n",
    "        return self._part_v\n",
    "    def part_o(self):\n",
    "        self._part_o == list(self.df[self.df.part_of_speech == '形容詞'].word)\n",
    "        return self._part_o\n",
    "    \n",
    "    def list_s2str(self):\n",
    "        self.str_s = str(self.part_s()).replace(\"', '\",\" \")[2:-2]\n",
    "        return self.str_s\n",
    "    def list_v2str(self):\n",
    "        self.str_v = str(self.part_v()).replace(\"', '\",\" \")[2:-2]\n",
    "        return self.str_v\n",
    "    def list_o2str(self):\n",
    "        self.str_o = str(self.part_o()).replace(\"', '\",\" \")[2:-2]\n",
    "        return self.str_o\n",
    "    \n",
    "class Word2value(Word):\n",
    "    def __init__(self,csv):\n",
    "        x_ls = [Word(i).list_s2str() for i in csv.X]\n",
    "        y_ls = list(csv.Y)\n",
    "\n",
    "        self.x_array = np.array(x_ls)\n",
    "        self.y_array = np.array(y_ls)\n",
    "        \n",
    "        self.cntvec = CountVectorizer()\n",
    "        x_cntvecs = self.cntvec.fit_transform(self.x_array)\n",
    "        self.x_cntararry = x_cntvecs.toarray()\n",
    "        \n",
    "        tfidf_vec = TfidfVectorizer(use_idf = True)\n",
    "        x_tfidf_vecs = tfidf_vec.fit_transform(self.x_array)\n",
    "        self.x_tfidf_array = x_tfidf_vecs.toarray()\n",
    "        \n",
    "    def len_all_term(self):\n",
    "        return len(self.x_tfidf_array[0])\n",
    "    def len_all_documents(self):\n",
    "        return len(self.x_tfidf_array)\n",
    "    def output_term_columns(self):\n",
    "        word_array = list(np.zeros(len(self.x_tfidf_array[0])))\n",
    "        for k,v in sorted(self.cntvec.vocabulary_.items(),key = lambda x:x[1]):\n",
    "            word_array[int(v)] = str(k)\n",
    "        return word_array\n",
    "    def IDF(self):\n",
    "        ## 総文書数 len(self.x_cntararry[:,i])\n",
    "        ##ある単語が出現する文書数 sum(self.x_cntararry[:,i] != 0) \n",
    "        tf = self.x_cntararry\n",
    "        self.idf = [np.log10(len(tf[:,i])/sum(tf[:,i] != 0) ) + 1 for i in range(len(tf[0]))]\n",
    "        return self.idf\n",
    "        \n",
    "    def TF(self):\n",
    "        return self.x_cntararry   ## columns = Term\n",
    "    \n",
    "    def TF_IDF(self): \n",
    "        ##TF-IDF値\n",
    "        '''\n",
    "            TFにIDF(Inverse Document Frequency)をかけたもの。\n",
    "            IDF値はlog(総文書数/ある単語が出現する文書数)\n",
    "        '''\n",
    "        return self.x_tfidf_array\n",
    "    \n",
    "\n",
    "## making of Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,data_len):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(data_len,256) ##線形変換(y = Ax + b)を行う bias = True バイアスを学習する\n",
    "        self.fc2 = nn.Linear(256,256)      ##入力データの件数、出力データの件数を引数とする\n",
    "        self.fc3 = nn.Linear(256,256)\n",
    "        self.fc4 = nn.Linear(256,128)\n",
    "        self.fc5 = nn.Linear(128,128)\n",
    "        self.fc6 = nn.Linear(128,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) ## ReLU_function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x) ##softmax_function\n",
    "\n",
    "def deep_learning(data1, data2):\n",
    "\n",
    "    '''\n",
    "    data1 = 説明変数(学習用)\n",
    "    data2 = 目的変数(学習用)\n",
    "    '''\n",
    "    Start = time.time() \n",
    "    # テンソルの作成　及び　ミニバッチの作成\n",
    "    ## test_data, train_dataの分類\n",
    "    train_x, test_x, train_y, test_y = model_selection.train_test_split(data1, data2, test_size = 0.2)\n",
    "\n",
    "    # テンソルの作成\n",
    "\n",
    "    ## train_data\n",
    "    Train_x = torch.from_numpy(train_x).float()\n",
    "    Train_y = torch.from_numpy(train_y).long()\n",
    "\n",
    "    ## teat_data\n",
    "    Test_x = torch.from_numpy(test_x).float()\n",
    "    Test_y = torch.from_numpy(test_y).long()\n",
    "\n",
    "    ## marge train_data(setumei, mokuteki)\n",
    "    train = TensorDataset(Train_x, Train_y)\n",
    "\n",
    "    ##separate minibatch\n",
    "    train_lorder = DataLoader(train, batch_size = 100, shuffle = True)\n",
    "\n",
    "    # モデルの学習\n",
    "\n",
    "    model = Net(len(data1[0])) ##model\n",
    "\n",
    "    ## Loss_function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    ##最適化関数のセット\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.01) ##lr = 学習率\n",
    "\n",
    "    ##start_learning\n",
    "    num = 1000 ##学習回数\n",
    "    start = time.time() \n",
    "    for epoch in range(num):\n",
    "        total_loss = 0\n",
    "        ##　take_out of sparated_data\n",
    "        for train_x, train_y in train_lorder:\n",
    "            ## Construction of Calculation graph\n",
    "            train_x, train_y = Variable(train_x),Variable(train_y)\n",
    "            ## reseting of Slope\n",
    "            optimizer.zero_grad()\n",
    "            ## Calculation of　propagation（順伝播）\n",
    "            output = model(train_x)\n",
    "            ## Calculation of loss\n",
    "            loss = criterion(output, train_y)\n",
    "            ## Calculation of　Counterpropagation（逆伝播）\n",
    "            loss.backward()\n",
    "            ##updata of weighting\n",
    "            optimizer.step()\n",
    "            ## Accumulation of loss\n",
    "            total_loss += loss.data[0]\n",
    "        #累積誤差を100回ごとに表示\n",
    "        if (epoch+1)%100 == 0:\n",
    "            end = time.time()\n",
    "            time_diff = end - start\n",
    "            print(epoch+1, total_loss, time_diff)\n",
    "            start = time.time() \n",
    "    print()\n",
    "    End = time.time()\n",
    "    Time_diff = End - Start\n",
    "    print(f'time = {Time_diff}')\n",
    "    print('finished learning')\n",
    "    print()\n",
    "\n",
    "    # 精度の確認\n",
    "    ## Construction of Calculation graph\n",
    "    test_x, test_y = Variable(Test_x), Variable(Test_y)\n",
    "\n",
    "    ##出力が０と１のどちらか\n",
    "    result = torch.max(model(test_x).data, 1)[1] ##入力テンソルの最大値を返す\n",
    "\n",
    "    ## モデルの精度を計算\n",
    "    accuray = sum(test_y.data.numpy() == result.numpy()) / len(test_y.data.numpy())\n",
    "    print(f'accuray = {accuray*100}%')\n",
    "    return model\n",
    "\n",
    "def Classification(model, array):\n",
    "    tensor = torch.from_numpy(array).float()\n",
    "    result = int(torch.max(model(tensor).data, 0)[1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataの選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/test_data.csv']\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "file = glob.glob('data/*.csv')\n",
    "print(file)\n",
    "csv = pd.read_csv(file[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文字の数値化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_value = Word2value(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cios</th>\n",
       "      <th>sdcv</th>\n",
       "      <th>コア</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cios     sdcv       コア\n",
       "0  0.57735  0.57735  0.57735\n",
       "1  0.57735  0.57735  0.57735\n",
       "2  0.57735  0.57735  0.57735"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "df_tfidf = pd.DataFrame(Word_value.TF_IDF())\n",
    "df_tfidf.columns = Word_value.output_term_columns()\n",
    "df_tfidf[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:131: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:190: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tensor(2.7449) 1.5175528526306152\n",
      "200 tensor(2.7452) 2.1921119689941406\n",
      "300 tensor(2.7448) 2.8987648487091064\n",
      "400 tensor(2.7448) 2.6683688163757324\n",
      "500 tensor(2.7448) 2.706009864807129\n",
      "600 tensor(2.7450) 2.7700982093811035\n",
      "700 tensor(2.7452) 2.875519037246704\n",
      "800 tensor(2.7448) 2.771466016769409\n",
      "900 tensor(2.7454) 2.7174429893493652\n",
      "1000 tensor(2.7448) 3.0694987773895264\n",
      "\n",
      "time = 26.207786798477173\n",
      "finished learning\n",
      "\n",
      "accuray = 61.0%\n"
     ]
    }
   ],
   "source": [
    "data1 = Word_value.x_tfidf_array\n",
    "data2 = Word_value.y_array\n",
    "\n",
    "model = deep_learning(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:131: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'hosomiとryoutaとコア'\n",
    "\n",
    "# 文字の数値化\n",
    "a = Word(text).list_s2str().split(' ')\n",
    "tf = [a.count(Word_value.output_term_columns()[i]) for i in range(Word_value.len_all_term())]\n",
    "tfidf = np.array(tf) * Word_value.IDF()\n",
    "\n",
    "## 分類\n",
    "result = Classification(model, tfidf)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
